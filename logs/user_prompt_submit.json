[
  {
    "session_id": "38cc65a3-09b0-4865-aafd-d4f65d638a61",
    "transcript_path": "/Users/crlough/.claude/projects/-Users-crlough-Code-personal-chat-pdf-via-rag/38cc65a3-09b0-4865-aafd-d4f65d638a61.jsonl",
    "cwd": "/Users/crlough/Code/personal/chat-pdf-via-rag",
    "permission_mode": "bypassPermissions",
    "hook_event_name": "UserPromptSubmit",
    "prompt": "Please take a look at this walkthrough I'm trying to follow and get it started. When I try to execute start up the app, it doesn't work: https://medium.com/@vndee.huynh/build-your-own-rag-and-run-it-locally-langchain-ollama-streamlit-181d42805895"
  },
  {
    "session_id": "38cc65a3-09b0-4865-aafd-d4f65d638a61",
    "transcript_path": "/Users/crlough/.claude/projects/-Users-crlough-Code-personal-chat-pdf-via-rag/38cc65a3-09b0-4865-aafd-d4f65d638a61.jsonl",
    "cwd": "/Users/crlough/Code/personal/chat-pdf-via-rag",
    "permission_mode": "bypassPermissions",
    "hook_event_name": "UserPromptSubmit",
    "prompt": "Please take a look at this walkthrough I'm trying to follow and get it started. When I try to execute start up the app, it doesn't work: \n\"\"\"\n\nSearch\nWrite\nSign up\n\nSign in\n\n\n\n\nWriting is for everyone.Register for Medium Day\n\n\nTop highlight\n\nBuild your own RAG and run it locally: Langchain + Ollama + Streamlit\nDuy Huynh\nDuy Huynh\n\nFollow\n6 min read\n\u00b7\nDec 5, 2023\n993\n\n\n16\n\n\n\nWith the rise of Large Language Models and its impressive capabilities, many fancy applications are being built on top of giant LLM providers like OpenAI and Anthropic. The myth behind such applications is the RAG framework, which has been thoroughly explained in the following articles:\n\nBuilding RAG-based LLM Applications for Production\nRetrieval Augmented Generation (RAG) Explained: Understanding Key Concepts\nWhat is retrieval-augmented generation?\nTo become familiar with RAG, I recommend going through these articles. This post, however, will skip the basics and guide you directly on building your own RAG application that can run locally on your laptop without any worries about data privacy and token cost.\n\nWe will build an application that is something similar to ChatPDF and EaseUS ChatPDF but simpler. Where users can upload a PDF document and ask questions through a straightforward UI. Our tech stack is super easy with Langchain, Ollama, and Streamlit.\n\nPress enter or click to view image in full size\n\nLLM Server: The most critical component of this app is the LLM server. Thanks to Ollama, we have a robust LLM Server that can be set up locally, even on a laptop. While llama.cpp is an option, I find Ollama, written in Go, easier to set up and run.\nRAG: Undoubtedly, the two leading libraries in the LLM domain are Langchain and LLamIndex. For this project, I\u2019ll be using Langchain due to my familiarity with it from my professional experience. An essential component for any RAG framework is vector storage. We\u2019ll be using Chroma here, as it integrates well with Langchain.\nChat UI: The user interface is also an important component. Although there are many technologies available, I prefer using Streamlit, a Python library, for peace of mind.\nOkay, let\u2019s start setting it up\n\nSetup Ollama\nAs mentioned above, setting up and running Ollama is straightforward. First, visit ollama.ai and download the app appropriate for your operating system.\n\nNext, open your terminal and execute the following command to pull the latest Mistral-7B. While there are many other LLM models available, I choose Mistral-7B for its compact size and competitive quality.\n\nollama pull mistral\nAfterward, run ollama list to verify if the model was pulled correctly. The terminal output should resemble the following:\n\nPress enter or click to view image in full size\n\nNow, if the LLM server is not already running, initiate it with ollama serve. If you encounter an error message like \"Error: listen tcp 127.0.0.1:11434: bind: address already in use\" it indicates the server is already running by default, and you can proceed to the next step.\n\nBuild RAG pipeline\nPress enter or click to view image in full size\n\nThe second step in our process is to build the RAG pipeline. Given the simplicity of our application, we primarily need two methods: ingest and ask. The ingest method accepts a file path and loads it into vector storage in two steps: first, it splits the document into smaller chunks to accommodate the token limit of the LLM; second, it vectorizes these chunks using Qdrant FastEmbeddings and store into Chroma. The ask method handles user queries. Users can pose a question, and then the RetrievalQAChain retrieves the relevant contexts (document chunks) using vector similarity search techniques. With the user's question and the retrieved contexts, we can compose a prompt and request a prediction from the LLM server.\n\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_community.embeddings import FastEmbedEmbeddings\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores.utils import filter_complex_metadata\n\n\nclass ChatPDF:\n    vector_store = None\n    retriever = None\n    chain = None\n\n    def __init__(self):\n        self.model = ChatOllama(model=\"mistral\")\n        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n        self.prompt = PromptTemplate.from_template(\n            \"\"\"\n            <s> [INST] Vous \u00eates un assistant pour les t\u00e2ches de r\u00e9ponse aux questions. Utilisez les \u00e9l\u00e9ments de contexte suivants pour r\u00e9pondre \u00e0 la question. \n            Si vous ne connaissez pas la r\u00e9ponse, dites simplement que vous ne savez pas.. Utilisez trois phrases\n             maximum et soyez concis dans votre r\u00e9ponse. [/INST] </s> \n            [INST] Question: {question} \n            Context: {context} \n            Answer: [/INST]\n            \"\"\"\n        )\n\n    def ingest(self, pdf_file_path: str):\n        docs = PyPDFLoader(file_path=pdf_file_path).load()\n        chunks = self.text_splitter.split_documents(docs)\n        chunks = filter_complex_metadata(chunks)\n\n        vector_store = Chroma.from_documents(documents=chunks, embedding=FastEmbedEmbeddings())\n        self.retriever = vector_store.as_retriever(\n            search_type=\"similarity_score_threshold\",\n            search_kwargs={\n                \"k\": 3,\n                \"score_threshold\": 0.5,\n            },\n        )\n\n        self.chain = ({\"context\": self.retriever, \"question\": RunnablePassthrough()}\n                      | self.prompt\n                      | self.model\n                      | StrOutputParser())\n\n    def ask(self, query: str):\n        if not self.chain:\n            return \"Please, add a PDF document first.\"\n\n        return self.chain.invoke(query)\n\n    def clear(self):\n        self.vector_store = None\n        self.retriever = None\n        self.chain = None\nThe prompt is sourced from the Langchain hub: Langchain RAG Prompt for Mistral. This prompt has been tested and downloaded thousands of times, serving as a reliable resource for learning about LLM prompting techniques. You can learn more about LLM prompting techniques here.\n\nGet Duy Huynh\u2019s stories in your inbox\nJoin Medium for free to get updates from this writer.\n\nEnter your email\nSubscribe\nMore details on the implementation:\n\ningest: We use PyPDFLoader to load the PDF file uploaded by the user. The RecursiveCharacterSplitter, provided by Langchain, then splits this PDF into smaller chunks. It's important to filter out complex metadata not supported by ChromaDB using the filter_complex_metadata function from Langchain. For vector storage, Chroma is used, coupled with Qdrant FastEmbed as our embedding model. This lightweight model is then transformed into a retriever with a score threshold of 0.5 and k=3, meaning it returns the top 3 chunks with the highest scores above 0.5. Finally, we construct a simple conversation chain using LCEL.\nask: This method simply passes the user's question into our predefined chain and then returns the result.\nclear: This method is used to clear the previous chat session and storage when a new PDF file is uploaded.\nDraft simple UI\nFor a simple user interface, we will use Streamlit, a UI framework designed for fast prototyping of AI/ML applications.\n\nimport os\nimport tempfile\nimport streamlit as st\nfrom streamlit_chat import message\nfrom rag import ChatPDF\n\nst.set_page_config(page_title=\"ChatPDF\")\n\n\ndef display_messages():\n    st.subheader(\"Chat\")\n    for i, (msg, is_user) in enumerate(st.session_state[\"messages\"]):\n        message(msg, is_user=is_user, key=str(i))\n    st.session_state[\"thinking_spinner\"] = st.empty()\n\n\ndef process_input():\n    if st.session_state[\"user_input\"] and len(st.session_state[\"user_input\"].strip()) > 0:\n        user_text = st.session_state[\"user_input\"].strip()\n        with st.session_state[\"thinking_spinner\"], st.spinner(f\"Thinking\"):\n            agent_text = st.session_state[\"assistant\"].ask(user_text)\n\n        st.session_state[\"messages\"].append((user_text, True))\n        st.session_state[\"messages\"].append((agent_text, False))\n\n\ndef read_and_save_file():\n    st.session_state[\"assistant\"].clear()\n    st.session_state[\"messages\"] = []\n    st.session_state[\"user_input\"] = \"\"\n\n    for file in st.session_state[\"file_uploader\"]:\n        with tempfile.NamedTemporaryFile(delete=False) as tf:\n            tf.write(file.getbuffer())\n            file_path = tf.name\n\n        with st.session_state[\"ingestion_spinner\"], st.spinner(f\"Ingesting {file.name}\"):\n            st.session_state[\"assistant\"].ingest(file_path)\n        os.remove(file_path)\n\n\ndef page():\n    if len(st.session_state) == 0:\n        st.session_state[\"messages\"] = []\n        st.session_state[\"assistant\"] = ChatPDF()\n\n    st.header(\"ChatPDF\")\n\n    st.subheader(\"Upload a document\")\n    st.file_uploader(\n        \"Upload document\",\n        type=[\"pdf\"],\n        key=\"file_uploader\",\n        on_change=read_and_save_file,\n        label_visibility=\"collapsed\",\n        accept_multiple_files=True,\n    )\n\n    st.session_state[\"ingestion_spinner\"] = st.empty()\n\n    display_messages()\n    st.text_input(\"Message\", key=\"user_input\", on_change=process_input)\n\n\nif __name__ == \"__main__\":\n    page()\nRun this code with the command streamlit run app.py to see what it looks like.\n\nPress enter or click to view image in full size\n\nOkay, that\u2019s it! We now have a ChatPDF application that runs entirely on your laptop. Since this post mainly focuses on providing a high-level overview of how to build your own RAG application, there are several aspects that need fine-tuning. You may consider the following suggestions to enhance your app and further develop your skills:\n\nAdd Memory to the Conversation Chain: Currently, it doesn\u2019t remember the conversation flow. Adding temporary memory will help your assistant be aware of the context.\nAllow multiple file uploads: it\u2019s okay to chat about one document at a time. But imagine if we could chat about multiple documents \u2014 you could put your whole bookshelf in there. That would be super cool!\nUse Other LLM Models: While Mistral is effective, there are many other alternatives available. You might find a model that better fits your needs, like LlamaCode for developers. However, remember that the choice of model depends on your hardware, especially the amount of RAM you have \ud83d\udcb5\nEnhance the RAG Pipeline: There\u2019s room for experimentation within RAG. You might want to change the retrieval metric, the embedding model,.. or add layers like a re-ranker to improve results.\nFinally, thank you for reading. If you find this information useful, please consider subscribing to my Substack or my personal blog. I plan to write more about RAG and LLM applications, and you\u2019re welcome to suggest topics by leaving a comment below. Cheers!\n\nFull source code: https://github.com/vndee/local-rag-example\n\nLlm\nChatbots\nAI\nMachine Learning\nLlama 2\n993\n\n\n16\n\n\nDuy Huynh\nWritten by Duy Huynh\n420 followers\n\u00b7\n12 following\nTechnician, AI/ML Engineer & Startups Builder. Checkout my personal site for more updated posts: https://blog.duy.dev/\n\n\nFollow\nResponses (16)\n\nWrite a response\n\nWhat are your thoughts?\n\nCancel\nRespond\nMadhusudhan Konda\nMadhusudhan Konda\n\nJan 25, 2024\n\n\nLECL\n\nLCEL - Langchain Expression Language\n8\n\n\n1 reply\n\nReply\n\nRavinthiran\nRavinthiran\n\nhim\nMay 1, 2024\n\n\nAny suggestions for running our own RAG using Langchain + Ollama in AWS cloud? Thank you :)\n8\n\nReply\n\nJuandlemusr\nJuandlemusr\n\nMar 29, 2024\n\n\nWhat if I get Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted. instead?\n2\n\n\n1 reply\n\nReply\n\nSee all responses\nMore from Duy Huynh\nBuild your own voice assistant and run it locally: Whisper + Ollama + Bark\nDuy Huynh\nDuy Huynh\n\nBuild your own voice assistant and run it locally: Whisper + Ollama + Bark\nAfter my latest post about how to build your own RAG and run it locally. Today, we\u2019re taking it a step further by not only implementing the\u2026\nMar 31, 2024\n604\n8\nLLM Sandbox: One Year Journey to 100k Downloads \ud83c\udf89\nLevel Up Coding\nIn\n\nLevel Up Coding\n\nby\n\nDuy Huynh\n\nLLM Sandbox: One Year Journey to 100k Downloads \ud83c\udf89\nHey everyone! \ud83d\udc4b\nJun 17\nThe Easiest Way to Add Code Interpreter into Your LLM Apps\nDuy Huynh\nDuy Huynh\n\nThe Easiest Way to Add Code Interpreter into Your LLM Apps\nLet me introduce you to LLM Sandbox, the simplest and most effective way to safely execute code generated by large language models (LLMs)\u2026\nJul 11, 2024\n41\n2\nBuilding a Database From Scratch: Understanding LSM Trees and Storage Engines (Part 1)\nLevel Up Coding\nIn\n\nLevel Up Coding\n\nby\n\nDuy Huynh\n\nBuilding a Database From Scratch: Understanding LSM Trees and Storage Engines (Part 1)\nLearn core database concepts by implementing a Python key-value store with crash recovery and efficient writes.\nOct 24, 2024\n54\nSee all from Duy Huynh\nRecommended from Medium\nAsync LLM Tasks with Celery and Celery Beat\nAlgoMart\nIn\n\nAlgoMart\n\nby\n\nYash Jain\n\nAsync LLM Tasks with Celery and Celery Beat\nUsing large language models in your backend? It\u2019s likely slow and blocks your app.\n\nMay 18\n91\nBuilding the Entire RAG Ecosystem and Optimizing Every Component\nLevel Up Coding\nIn\n\nLevel Up Coding\n\nby\n\nFareed Khan\n\nBuilding the Entire RAG Ecosystem and Optimizing Every Component\nRouting, Indexing, Retrieval, Transformation and more.\n\nAug 10\n1.7K\n22\nImplementing RAG using Langchain, Ollama and Qdrant\nSpandan Maity\nSpandan Maity\n\nImplementing RAG using Langchain, Ollama and Qdrant\nIn this post, we will implement the RAG (Retrieval-Augmented Generation) by using Langchain, Ollama and Qdrant as a vector store.\nMar 24\n14\nBuilding Agentic RAG with LangGraph: Mastering Adaptive RAG for Production\nArtificial Intelligence in Plain English\nIn\n\nArtificial Intelligence in Plain English\n\nby\n\nPiyush Agnihotri\n\nBuilding Agentic RAG with LangGraph: Mastering Adaptive RAG for Production\nBuild intelligent RAG systems that know when to retrieve documents, search the web, or generate responses directly\n\nJul 20\n1.8K\n27\nBringing AI to Apache Superset: Integrating Chatbots for Real\u2011Time Insights\nSundaram Dubey\nSundaram Dubey\n\nBringing AI to Apache Superset: Integrating Chatbots for Real\u2011Time Insights\nIn today\u2019s data\u2011driven world, static dashboards are no longer enough. Business users want to ask questions in natural language\u200a\u2014\u200a\u201cWhat was\u2026\n\nApr 12\n8\n2\nSimple RAG Application with Ollama and Langgraph\nvdcapriles\nvdcapriles\n\nSimple RAG Application with Ollama and Langgraph\nTempted to use one of the AI models available for work but you can\u2019t due to data privacy concerns? Well, then this article might help you\u2026\n\nMar 15\n149\nSee more recommendations\nHelp\n\nStatus\n\nAbout\n\nCareers\n\nPress\n\nBlog\n\nPrivacy\n\nRules\n\nTerms\n\nText to speech\n\"\"\""
  }
]